services:
  ollama:
    # https://hub.docker.com/r/ollama/ollama
    # https://github.com/ollama/ollama
    #image: ollama/ollama:latest
    # AMD - BEGIN
    # Comment the NVIDIA part and the image above
    # Uncomment the lines below
    image: ollama/ollama:0.3.7-rc5-rocm
    devices:
      - /dev/kfd
      - /dev/dri
    environment:
    #  - OLLAMA_DEBUG=1
      - HSA_OVERRIDE_GFX_VERSION=10.3.0
      - ROCR_VISIBLE_DEVICES=1
    # AMD - END
    pull_policy: always
    container_name: ollama
    # Uncomment below to expose Ollama API outside the container stack
    #ports:
    #  - 11434:11434
    volumes:
      - ./data/ollama:/root/.ollama
    tty: true
    # NVIDIA - BEGIN
    # Uncomment this part if you have a NVIDIA graphics card and
    # you want to use it with Ollama
    # For more information https://hub.docker.com/r/ollama/ollama and check
    # Nvidia GPU
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: all
    #          capabilities: [gpu]
    # NVIDIA - END
    restart: unless-stopped

  ollama-webui:
    # https://github.com/open-webui/open-webui
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    depends_on:
      - ollama
    ports:
      - 3000:8080
    environment:
      - "OLLAMA_BASE_URL=http://ollama:11434"
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - ./data/open-webui:/app/backend/data
    restart: unless-stopped
